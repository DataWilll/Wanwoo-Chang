# 데이터 전처리 계획

## ●현 데이터 분석 과정에서 발생하는 문제점

### 1. 분석 결과 ⇒ 해석 ?

▶ BC카드의 MZ세대와 비 MZ세대를 구분하는 모델 정확도가 0.84가 나왔고 모델의 feature importance가 가구생애주기가 가장 높게 나타났다.

⇒ 이를 통해 알아낼 수 있는것이 무엇일까?

MZ세대 자체의 특징은 이미 인터넷 상에서 그 특징이 다뤄졌고 오히려 M세대와 Z세대를 구분짓고자 함

## 

### 2. 데이터 셋의 시간(기준년월)을 무시해도 상관이 없을까?

▶ KB국민카드의 활용사례 내용

1. 지역별(상권별) 떠오르는 or 지는 업종 및 요식업 트랜드 분석 

2. 요식업 메뉴(트랜드)별 지속기간(장, 단기) 및 성별,연령별 분포 차이 등 

<분석방향> - 동일한 법정동 단위 공공데이터 상의 지역별 특성과 결합(ex. 지하철 수, 버스정류장 수, 학교 수 등) 가능 - 

법정동을 결합하여 특정 골목 상권 지칭 가능 및 시각화 적극 활용 요망 <참고사항>

▶ 신한은행 활용사례

 - 코로나 전후의 금융 거래 변동 분석

 - 시계열 분석(2019년 ~ 2021년 )을 통한 코로나 전후 서울시 금융 정보 변동

 - 코로나로 인한 경제활동 영향 분석 - 연령대별 소득/부채의 변동 관계 및 자산 증감 분석

물론 이를 무조건 따라갈 필요는 없지만 제공 데이터 자체가 어느정도 시간을 고려할 것을 요구하고 있다.

▶데이터 전처리를 어느정도로 해야하는가?

데이터가 어느정도 전처리가 되어있지만 특징이 나타나지 않는 무난한 데이터이다. 이를 특징이 있게 만들어 줄 필요가 있다. (데이터 전처리가 90%)

## ● 데이터 분석의 방향?

### 1. 데이터를 간단하고 간결하게 정리

ex) 헬스장 탈퇴 또는 지속 이용 고객 분류 데이터 

월별 고객의 방문횟수 가입일/ 탈퇴일

고객의 월 방문 횟수의 평균, 최대, 최소값을 계산 ⇒ 월 최대 4회이상 (월~일 중 특정 요일에 4번이상 사용했으면 주기적인 이용고객으로 )

가입일과 탈퇴일로 회원 기간 생성 등

어느정도 데이터를 더 특징있게 구분할 필요가 있을 것으로 보임 

### 2. 대상을 20대와 30대로 한정

결론적으로 제출하고자하는 내용은 MZ세대에 해당하는 20대와 30대이므로 대상을 이들로 한정하는 것이 특징 짓기에 더 적합하다고 판단

또한 MZ세대와 비 MZ세대를 구분 짓는 내용은 많은 특징이 이미 나와있음

1. 다양한 만남 추구
2. 후렌드(who+friend)누구와도 친구가 될 수 있음
3. 선취력(원하는 바를 얻기 위해서 행동)
4. 판플레이(참여하여 경험하는 컨텐츠 선호)
5. 클라우드 소비(소유 소비 보다는 경험 공유를 선호)
6. 문화 소비를 선호
7. 사회이슈에 민감 (윤리적 이슈에 가장 민감하게 반응함 → 쿠팡 화재 → 쿠팡 탈퇴)
8. 자신이 얻고자 하는 것에는 과감한 소비 (플렉스) + 소소한 지출은 절약 (야누스적 소비)

## ●고려해야할 점

### 1. 한국투자증권 데이터

한국 투자증권 데이터의 경우에는 다른 데이터와 달리 20년 1월부터 20년 3월까지로 상대적으로 짧은 기간의 데이터를 가지고 있다. 따라서 기간을 고려한 분석을 실시하게 되면 다른 데이터와는 다르게 해석이 되어야함

### 

### 2. KB국민카드 데이터의 순위컬럼

KB 국민카드의 데이터 순위칼럼은 년월 특정 시/동/ 업종별 매출건수 순위를 의미

2019년 3월 경기 구미동 한식 면요리 국수 2위는 

즉, 2019년 3월 경기 구미동 한식 면요리 국수 업종이 해당지역에서 매출건수가 두번째로 많았다는 것을 의미한다.

다시말해 특정 기간 하나의 법정동 내에서 마케팅업종세세분류별 매출건수 순위

```python
import numpy as np
a=df2.loc[(df2.날짜=='2019-03')&(df2.한글시군구명=='성남시 분당구')&(df2.법정동리명=='구미동'),'순위'].unique()

b=df2.loc[(df2.날짜=='2019-03')&(df2.한글시군구명=='성남시 분당구')&(df2.법정동리명=='구미동')&(df2.마케팅업종중분류명=='양식'),'순위'].unique()
c=df2.loc[(df2.날짜=='2019-03')&(df2.한글시군구명=='성남시 분당구')&(df2.법정동리명=='구미동')&(df2.마케팅업종중분류명=='한식'),'순위'].unique()
d=df2.loc[(df2.날짜=='2019-03')&(df2.한글시군구명=='성남시 분당구')&(df2.법정동리명=='구미동')&(df2.마케팅업종중분류명=='일식'),'순위'].unique()

print(np.sort(a))
print(np.sort(b))
print(np.sort(c))
print(np.sort(d))

[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
[3 4]
[ 1  2  5  6  7 10 11 12 13 14 16 17 18]
[ 8  9 15]
```

## 그래서 분석은 어떻게?

### ◆원래 생각

원래 처음 생각했던 것은 최신 데이터 (21년 3월 데이터)를 먼저 기초 통계량 분석과 클러스터링을 통해서  최신데이터의 특징을 발견하고 나머지 (19,20년 데이터)로 모델링을 실시 후 (fit / predict) 최신 데이터를 모델에 도입했을 때 정확도나 feature_importance에 어떠한 변화가 있는가

그 후 모델에 최신 데이터의 특징에 따른 가중치를 부여하여 재 모델링을 해서 다시 예측했을때 정확도가 올라갔다면 해당 특징이 최신데이터에서 더 강한 영향력을 미쳤다고 해석할 수 있지 않을까?라고 생각했었음

⇒ 이 경우  최신의 데이터가 20년의 데이터와 크게 다르지 않을 수도 있기 때문에 별다른 insight를 발견하지 못할 수도 있음

◆LSTM

LSTM분석 신경망 분석으로 감성분석에 많이 쓰이지만 회귀나 분류에도 많이 사용된다. 이 분석의 경우 시계열 분석에 적합하다. (다만 내용이 배운게 아니고 keras를 사용)

◆코로나 전후

코로나 전후를 기준으로 나누어서 생각하는 것도 가능하다.

코로나 이전 데이터를 가지고 모델링을 실시 ⇒ 최신 데이터를 대입하여 분류 실시

코로나 이후 데이터를 기존 모델링에 추가 ⇒ 최신데이터를 대입하여 분류 실시

정확도와 feature_importance 등을 이용해서 변화한 내용을 찾아내어 어떠한 변화가 있었는지 분석하는 것도 의미있는 분석이 될 수 있다.

한전 → 원자재 사용량 예측시 기존의 데이터에 새로운 데이터를 지속적으로 업데이트 누적시켜서 다음 사용량을 예측한다.